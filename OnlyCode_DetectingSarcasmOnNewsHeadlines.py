# -*- coding: utf-8 -*-
"""TextMining Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y_bMeshT9jexLazV9g6gZkPJCKwdgmGi
"""

!pip install transformers
!pip install --upgrade spacy

import pandas as pd
import spacy
from keras.callbacks import EarlyStopping, ModelCheckpoint
from numpy import savetxt, loadtxt
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics import classification_report
from sklearn.pipeline import Pipeline
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV
import numpy as np
import gensim
import keras
from keras.preprocessing import text, sequence
from keras.models import Sequential, load_model
from keras.layers import Input, Dense, Embedding, LSTM, Dropout, Bidirectional, Conv1D, GlobalMaxPool1D
import tensorflow as tf
import gensim.downloader
from transformers import TFBertModel, BertTokenizer
import torch
from tokenizers import BertWordPieceTokenizer

path = '/content/drive/MyDrive/ColabNotebooks/TextMiningProject/TextMiningData/'

# import original data files
data_1 = pd.read_json(path+"Sarcasm_Headlines_Dataset.json", lines=True)
data_2 = pd.read_json(path+"Sarcasm_Headlines_Dataset_v2.json", lines=True)

data_all = pd.concat([data_1, data_2], axis=0, ignore_index=True)
data_all = data_all.drop_duplicates(subset=['headline'])
data_all = data_all.drop(['article_link'], axis=1)
data_all = data_all.sample(frac=1, random_state=0)

len(data_all)  # 28503
sum(data_all['is_sarcastic'] == 0) / len(data_all)  # 0.5245

data_2.head()

!python -m spacy download en_core_web_lg

# tokenize the data!
nlp = spacy.load('en_core_web_lg')

def preprocess(text):
    doc = nlp(text)
    res = []
    for token in doc:
        if token.is_stop != True and token.is_alpha == True:
            res.append(token.text.lower())
    return " ".join(res)

data_all['headline_before'] = data_all['headline']
data_all['headline'] = data_all['headline_before'].apply(preprocess)

empty_headlines_idx = np.where(data_all['headline'].apply(lambda x: x == ''))
data_all.iloc[empty_headlines_idx]
#                     headline  is_sarcastic
# 9295     i was, but now i am             0
# 19626      what's in a name?             0
# 25158  who were you on 9/11?             0
# 26311         you are enough             0
data_tokenized = data_all.drop_duplicates(subset=['headline'])
data_tokenized = data_all[data_all.headline != '']
# save tokenized data
data_tokenized.to_json(r'Sarcasm_Headlines_Dataset_tokenized.json', orient='records')
len(data_tokenized)  # 28464

# read tokenized data
data_tokenized = pd.read_json(path+"Sarcasm_Headlines_Dataset_tokenized.json")

data_tokenized[:10]

# visualize the data
fig, ax = plt.subplots()
ax.pie([sum(data_tokenized['is_sarcastic'] == 0), sum(data_tokenized['is_sarcastic'] == 1)],
        explode=(0,0.05), labels=["Non-sarcastic","Sarcastic"], autopct='%1.1f%%',
        shadow=True, startangle=90)
ax.axis('equal')
plt.show()
fig.savefig(path+"class distribution.png")

# visualize the data
sum(data_tokenized['is_sarcastic'] == 0) / len(data_tokenized)  # 0.5239
data_headline_sar = data_tokenized[data_tokenized['is_sarcastic'] == 1]['headline']
data_headline_non_sar = data_tokenized[data_tokenized['is_sarcastic'] == 0]['headline']
# word counts distribution in each class
text_len_sar = data_headline_sar.map(lambda x: len(x.split()))
text_len_non_sar = data_headline_non_sar.map(lambda x: len(x.split()))
fig, ax = plt.subplots(1, 2, figsize=(10, 5))
ax[0].set_title('Word count in sarcastic text')
ax[0].hist(text_len_sar, range=(0, 18))  # exclude 6
ax[1].set_title('Word count in non-Sarcastic text')
ax[1].hist(text_len_non_sar, range=(0, 18))  # exclude 1
plt.show()
fig.savefig(path+"Words in text.png")

# average word length of each text distribution in each class
word_len_sar = data_headline_sar.map(lambda x: np.mean([len(i) for i in x.split()]))
word_len_non_sar = data_headline_non_sar.map(lambda x: np.mean([len(i) for i in x.split()]))
fig, ax = plt.subplots(1, 2, figsize=(10, 5))
ax[0].set_title('Average word length in sarcastic text')
ax[0].hist(word_len_sar)
ax[1].set_title('Average word length in non-sarcastic text')
ax[1].hist(word_len_non_sar)
plt.show()
fig.savefig(path+"Average word length in text.png")

# visualize the data
words_sar = data_headline_sar.str.split(expand=True).unstack().value_counts(ascending=True)
words_nonsar = data_headline_non_sar.str.split(expand=True).unstack().value_counts(ascending=True)
fig, ax = plt.subplots(1, 2, figsize=(11, 10))
ax[0].set_title('Words frequency in sarcastic text')
ax[0].barh(range(50), words_sar.values[-50:], tick_label=words_sar.keys()[-50:])
ax[1].set_title('Words frequency in non-sarcastic text')
ax[1].barh(range(50), words_nonsar.values[-50:], tick_label=words_nonsar.keys()[-50:])
plt.show()
fig.savefig(path+"Words frequency in text.png")

# split data into training, validation, test dataset
data_trainval, data_test = train_test_split(data_tokenized, test_size=0.15, random_state=0)
data_train, data_val = train_test_split(data_trainval, test_size=0.2, random_state=0)

print(len(data_train))  # 19355
print(len(data_val))  # 4839
print(len(data_test))  # 4270

# baselines
# naive bayes classifier - 79%
pipe = Pipeline([('vectorizer', CountVectorizer()), ('MNB', MultinomialNB())])
pipe.fit(data_train['headline'], data_train['is_sarcastic'])
nb_pred = pipe.predict(data_test['headline'])
print(classification_report(data_test['is_sarcastic'], nb_pred))
# show confusion matrix
cm = confusion_matrix(data_test['is_sarcastic'], nb_pred, labels=[0, 1])
display = ConfusionMatrixDisplay(confusion_matrix=cm,
                                 display_labels=["non-sarcastic", "sarcastic"])
display.plot()
plt.show()

# CV
parameters = {'vectorizer__binary': [False, True],
              'vectorizer__ngram_range': [(1, 1), (1, 2), (2, 2)],
              'MNB__alpha': [5, 2, 1]}
clf = GridSearchCV(pipe, parameters, cv=5)
clf.fit(data_trainval['headline'], data_trainval['is_sarcastic'])
print(f"The best model is found using binary = {clf.best_params_['vectorizer__binary']}, "
      f"ngram_range = {clf.best_params_['vectorizer__ngram_range']}, "
      f"alpha = {clf.best_params_['MNB__alpha']} with average accuracy of {round(clf.best_score_, 3)}.")

# Try TfidfVectorizer
# naive bayes classifier - 79%
pipe = Pipeline([('vectorizer', TfidfVectorizer()), ('MNB', MultinomialNB())])
pipe.fit(data_train['headline'], data_train['is_sarcastic'])
nb_pred = pipe.predict(data_test['headline'])
print(classification_report(data_test['is_sarcastic'], nb_pred))
# show confusion matrix
cm = confusion_matrix(data_test['is_sarcastic'], nb_pred, labels=[0, 1])
display = ConfusionMatrixDisplay(confusion_matrix=cm,
                                 display_labels=["non-sarcastic", "sarcastic"])
display.plot()
plt.show()

# with count vectorizer
# After CV - 0.80
pipe = Pipeline([('vectorizer', CountVectorizer(binary=True, ngram_range=(1, 2))),
                 ('MNB', MultinomialNB())])
pipe.fit(data_train['headline'], data_train['is_sarcastic'])
nb_pred = pipe.predict(data_test['headline'])
print(classification_report(data_test['is_sarcastic'], nb_pred))
# show confusion matrix
cm = confusion_matrix(data_test['is_sarcastic'], nb_pred, labels=[0, 1])
display = ConfusionMatrixDisplay(confusion_matrix=cm,
                                 display_labels=["non-sarcastic", "sarcastic"])
display.plot()
plt.show()
display.im_.figure.savefig(path+"naive bayes cm.png")

# Methodology
# LSTM
tokenizer = text.Tokenizer(num_words=35000)
tokenizer.fit_on_texts(data_train.headline)  # only train
tokenized_train = tokenizer.texts_to_sequences(data_train.headline)
x_train_seq = sequence.pad_sequences(tokenized_train, maxlen=20)
tokenized_val = tokenizer.texts_to_sequences(data_val.headline)  # only val
x_val_seq = sequence.pad_sequences(tokenized_val, maxlen=20)
tokenized_test = tokenizer.texts_to_sequences(data_test.headline)  # only test
x_test_seq = sequence.pad_sequences(tokenized_test, maxlen=20)

size_of_vocabulary = min(35000, len(tokenizer.word_index) + 1)  # 21736   +1 for UNKNOWN

# create wordembedding matrix
# # Glove glove.6B.200d
# # load glove word vectors
# embeddings_index = dict()
# f = open('glove.6B/glove.6B.200d.txt')
# for line in f:
#     values = line.split()
#     embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')
# f.close()
# print('Loaded %s word vectors from GloVe.' % len(embeddings_index))  # 400,000
# # create a weight matrix for words in training data
embedding_matrix = np.zeros((size_of_vocabulary, 200))  # 21736x200
# for word, i in tokenizer.word_index.items():
#     if i >= 35000:
#         continue
#     embedding_vector = embeddings_index.get(word)
#     if embedding_vector is not None:
#         embedding_matrix[i] = embedding_vector
# # glove glove.6B.200d end

# gensim - glove-twitter-200
# print(list(gensim.downloader.info()['models'].keys()))
glove_vectors = gensim.downloader.load('glove-twitter-200')
print('Loaded %s word vectors from GloVe.' % len(glove_vectors))  # 1,193,514
# create a weight matrix for words in training data
for word, i in tokenizer.word_index.items():
    if i >= 35000:
        continue
    if word in glove_vectors.key_to_index:
        embedding_matrix[i] = glove_vectors[word]
# gensim - glove-twitter-200 end

# save embedding_matrix to csv file
savetxt(path+'embedding_matrix_glovetwitter200.csv', embedding_matrix, delimiter=',')

# load array
# embedding_matrix = loadtxt('embedding_matrix_glove6B200d.csv', delimiter=',', dtype='float32')
embedding_matrix = loadtxt(path+'embedding_matrix_glovetwitter200.csv', delimiter=',', dtype='float32')

# function to plot model training process
def plot_modelfit_process(model_history, title):
  n = len(model_history.history['acc'])
  fig, ax = plt.subplots(2, 1, figsize=(8, 8))
  ax[0].set_title('Accuracy')
  ax[0].plot(model_history.history['acc'])
  ax[0].plot(model_history.history['val_acc'])
  ax[0].axvline(x=np.where(model_history.history['val_acc'] == np.max(model_history.history['val_acc']))[0][0],
                color='k', linestyle='--')
  ax[0].set_xlabel("Epochs")
  ax[0].set_xticks(range(n))
  ax[0].legend(['train', 'validation'])

  ax[1].set_title('Loss')
  ax[1].plot(model_history.history['loss'])
  ax[1].plot(model_history.history['val_loss'])
  ax[1].axvline(x=np.where(model_history.history['val_loss'] == np.min(model_history.history['val_loss']))[0][0],
                color='k', linestyle='--')
  ax[1].set_xlabel("Epochs")
  ax[1].set_xticks(range(n))
  ax[1].legend(['train', 'validation'])
  fig.tight_layout(pad=2.0)
  plt.show()
  fig.savefig(path+title)

# Defining Neural Network
model = Sequential()
# Non-trainable embedding layer
model.add(Embedding(input_dim=size_of_vocabulary, output_dim=200, weights=[embedding_matrix],
                    input_length=20, trainable=False))
# LSTM
model.add(Bidirectional(LSTM(units=128,
                             return_sequences=True,
                             dropout=0.5)))
model.add(GlobalMaxPool1D())
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.35))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer=tf.keras.optimizers.Adam(),
              loss='binary_crossentropy', metrics=['acc'])
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)
mc = ModelCheckpoint(path+'best_model_lstm1.h5', monitor='val_acc', mode='max',
                     save_best_only=True, verbose=1)

model.summary()

# train
history = model.fit(x_train_seq, data_train.is_sarcastic,
                    batch_size=128,
                    validation_data=(x_val_seq, data_val.is_sarcastic),
                    verbose=1,
                    epochs=20,
                    callbacks=[es,mc])

# evaluation
model = load_model(path+'best_model_lstm.h5')
_, test_acc = model.evaluate(x_test_seq, data_test.is_sarcastic, batch_size=128)
print("Accuracy of the model on Testing Data is - ", test_acc * 100)
model_pred_lstm = model.predict(x_test_seq, batch_size=128)
lstm_pred = np.where(model_pred_lstm >= 0.5, 1, 0)

# plot training accuracy
plot_modelfit_process(history, "training process lstm.png")

# show confusion matrix
cm = confusion_matrix(data_test.is_sarcastic, lstm_pred, labels=[0, 1])
display = ConfusionMatrixDisplay(confusion_matrix=cm,
                                 display_labels=["non-sarcastic", "sarcastic"])
display.plot()
plt.show()
display.im_.figure.savefig(path+"lstm cm.png")

print(classification_report(data_test.is_sarcastic, lstm_pred))

# Bert
# tokenize texts by using pre-trained bert tokenizer
fast_tokenizer = BertWordPieceTokenizer(path+'bert-base-uncased-vocab.txt', lowercase=True)

def fast_encode(texts, tokenizer, chunk_size=256, maxlen=20):
    tokenizer.enable_truncation(max_length=maxlen)
    tokenizer.enable_padding(length=maxlen)
    all_ids = []
    for i in range(0, len(texts), chunk_size):
        text_chunk = texts[i:i + chunk_size].tolist()
        encs = tokenizer.encode_batch(text_chunk)
        all_ids.extend([enc.ids for enc in encs])
    return np.array(all_ids)

x_train_encode = fast_encode(data_train.headline, fast_tokenizer, maxlen=20)
x_val_encode = fast_encode(data_val.headline, fast_tokenizer, maxlen=20)
x_test_encode = fast_encode(data_test.headline, fast_tokenizer, maxlen=20)

# build model using pre-trained BERT model
bert_model_pre_trained = TFBertModel.from_pretrained('bert-base-uncased')
bert_input_word_ids = Input(shape=(20,), dtype=tf.int32, name="input_word_ids")
bert_embedding = bert_model_pre_trained(bert_input_word_ids)[0]
cls_token = tf.keras.layers.GlobalAveragePooling1D()(bert_embedding) #bert_embedding[:, 0, :]
bert_output = Dense(1, activation='sigmoid')(cls_token)
bert_model = tf.keras.Model(inputs=bert_input_word_ids, outputs=bert_output)
bert_model.compile(tf.keras.optimizers.Adam(learning_rate=2e-5),
                   loss='binary_crossentropy', metrics=['acc'])
bert_model.summary()

# train
bert_mc = ModelCheckpoint(path+'best_model_bert1.h5', monitor='val_acc', save_weights_only=True, save_best_only=True, verbose=1)
bert_es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)

bert_history = bert_model.fit(x_train_encode, data_train.is_sarcastic,
                         batch_size=128,
                         validation_data=(x_val_encode, data_val.is_sarcastic),
                         epochs=10,
                         callbacks=[bert_mc, bert_es])

# evaluation
bert_model.load_weights(path+'best_model_bert.h5')
bert_model.compile(tf.keras.optimizers.Adam(learning_rate=2e-5),
                   loss='binary_crossentropy', metrics=['acc'])
_, test_acc = bert_model.evaluate(x_test_encode, data_test.is_sarcastic, batch_size=128)
print("Accuracy of the model on Testing Data is - ", test_acc * 100)
# 0.85878 - best_model_bert
model_pred_bert = bert_model.predict(x_test_encode, batch_size=128)
bert_pred = np.where(model_pred_bert >= 0.5, 1, 0)

# plot training accuracy
plot_modelfit_process(bert_history, "training process bert.png")
# show confusion matrix
cm = confusion_matrix(data_test.is_sarcastic, bert_pred, labels=[0, 1])
display = ConfusionMatrixDisplay(confusion_matrix=cm,
                                 display_labels=["non-sarcastic", "sarcastic"])
display.plot()
plt.show()
display.im_.figure.savefig(path+"bert cm.png")
# classification report
print(classification_report(data_test.is_sarcastic, bert_pred))

#comparison
nonsar_nb = [0.81,0.81,0.81]
sar_nb = [0.79,0.79,0.79]
f1_nb = 0.80
acc_nb = 0.80

nonsar_lstm = [0.84,0.83,0.83]
sar_lstm = [0.81,0.83,0.82]
f1_lstm = 0.83
acc_lstm = 0.8283372521400452

nonsar_bert = [0.86,0.87,0.87]
sar_bert = [0.86,0.84,0.85]
f1_bert = 0.86
acc_bert = 0.858782172203064

colors = ('#78C8F6','#069AF3','#166688')
labels = ("Naive Bayes", "LSTM", "BERT")

plt.figure(figsize=(4, 5))
plt.bar(range(3), (f1_nb, f1_lstm, f1_bert), tick_label=labels, color=colors, alpha=0.7)
plt.title('F1 score')
plt.savefig(path+'all F1 score.png', dpi=100)
plt.show()

plt.figure(figsize=(4, 5))
plt.bar(range(3), (acc_nb, acc_lstm, acc_bert), tick_label=labels, color=colors, alpha=0.7)
plt.title('Accuracy')
plt.savefig(path+'all Accuracy.png', dpi=100)
plt.show()

# Construct hybrid model from article
# Defining Neural Network
model = Sequential()
# Non-trainable embedding layer
model.add(Embedding(input_dim=size_of_vocabulary, output_dim=200, weights=[embedding_matrix],
                    input_length=20, trainable=False))
# LSTM-CNN
model.add(Bidirectional(LSTM(units=128,
                             return_sequences=True,
                             dropout=0.35)))
model.add(Conv1D(filters=64, kernel_size=3))
model.add(tf.keras.layers.MaxPooling1D(pool_size=2))
model.add(Dropout(0.35))
model.add(tf.keras.layers.Flatten())
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer=tf.keras.optimizers.Adam(),
              loss='binary_crossentropy', metrics=['acc'])
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)
mc = ModelCheckpoint(path+'best_model_lstm22.h5', monitor='val_acc', mode='max',
                     save_best_only=True, verbose=1)

model.summary()

# train
history = model.fit(x_train_seq, data_train.is_sarcastic,
                    batch_size=128,
                    validation_data=(x_val_seq, data_val.is_sarcastic),
                    verbose=1,
                    epochs=20,
                    callbacks=[es,mc])

# evaluation
#model = load_model(path+'best_model_lstm22.h5')
_, test_acc = model.evaluate(x_test_seq, data_test.is_sarcastic, batch_size=128)
print("Accuracy of the model on Testing Data is - ", test_acc * 100)
model_pred = model.predict(x_test_seq, batch_size=128)
lstm_pred = np.where(model_pred >= 0.5, 1, 0)

# plot training accuracy
plot_modelfit_process(history, "training process lstm compare.png")

# show confusion matrix
cm = confusion_matrix(data_test.is_sarcastic, lstm_pred, labels=[0, 1])
display = ConfusionMatrixDisplay(confusion_matrix=cm,
                                 display_labels=["non-sarcastic", "sarcastic"])
display.plot()
plt.show()
display.im_.figure.savefig(path+"lstm cm compare.png")

print(classification_report(data_test.is_sarcastic, lstm_pred))

# analyse come wrongly classified observations
# LSTM model
df_wrong_lstm = data_test[lstm_pred[:,0]!=data_test.is_sarcastic]
#model_pred_lstm

# BERT model
df_wrong_bert = data_test[bert_pred[:,0]!=data_test.is_sarcastic]
#model_pred_bert

# both did wrong
index_wrong = []
for i in df_wrong_bert.index:
  if i in df_wrong_lstm.index:
    index_wrong.append(i)

df_wrong_both = data_test.loc[index_wrong]

df_wrong_both

df_wrong_lstm

df_wrong_bert

